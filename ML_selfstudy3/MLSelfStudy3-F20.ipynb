{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Self Study 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this self study we perform character recognition using SVM classifiers. We use the MNIST dataset, which consists of 70000 handwritten digits 0..9 at a resolution of 28x28 pixels. \n",
    "\n",
    "Stuff we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.datasets import fetch_openml  ##couldn't run with the previous code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the MNIST data. Using the fetch_mldata function, this will be downloaded from the web, and stored in the directory you specify as data_home (replace my path in the following cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(name='mnist_784', data_home='/home/starksultana/Documentos/Mestrado_4o ano/2o sem AAU/ML/ML_selfstudy3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has .data and .target attributes. The following gives us some basic information on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 70000\n",
      "\n",
      "Number of features: 784\n",
      "\n",
      "features:  [[  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  3.  18.  18.  18.]\n",
      " [126. 136. 175.  26.]\n",
      " [166. 255. 247. 127.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [ 30.  36.  94. 154.]\n",
      " [170. 253. 253. 253.]\n",
      " [253. 253. 225. 172.]\n",
      " [253. 242. 195.  64.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.  49.]\n",
      " [238. 253. 253. 253.]\n",
      " [253. 253. 253. 253.]\n",
      " [253. 251.  93.  82.]\n",
      " [ 82.  56.  39.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.  18.]\n",
      " [219. 253. 253. 253.]\n",
      " [253. 253. 198. 182.]\n",
      " [247. 241.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [ 80. 156. 107. 253.]\n",
      " [253. 205.  11.   0.]\n",
      " [ 43. 154.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.  14.   1. 154.]\n",
      " [253.  90.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0. 139.]\n",
      " [253. 190.   2.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.  11.]\n",
      " [190. 253.  70.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [ 35. 241. 225. 160.]\n",
      " [108.   1.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.  81. 240. 253.]\n",
      " [253. 119.  25.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.  45. 186.]\n",
      " [253. 253. 150.  27.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.  16.]\n",
      " [ 93. 252. 253. 187.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0. 249. 253. 249.]\n",
      " [ 64.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.  46. 130.]\n",
      " [183. 253. 253. 207.]\n",
      " [  2.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [ 39. 148. 229. 253.]\n",
      " [253. 253. 250. 182.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.  24. 114.]\n",
      " [221. 253. 253. 253.]\n",
      " [253. 201.  78.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [ 23.  66. 213. 253.]\n",
      " [253. 253. 253. 198.]\n",
      " [ 81.   2.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.  18. 171.]\n",
      " [219. 253. 253. 253.]\n",
      " [253. 195.  80.   9.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [ 55. 172. 226. 253.]\n",
      " [253. 253. 253. 244.]\n",
      " [133.  11.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [136. 253. 253. 253.]\n",
      " [212. 135. 132.  16.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]]\n",
      "List of labels: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of datapoints: {}\\n\".format(mnist.data.shape[0]))\n",
    "print(\"Number of features: {}\\n\".format(mnist.data.shape[1]))\n",
    "print(\"features: \", mnist.data[0].reshape(196,4))\n",
    "print(\"List of labels: {}\\n\".format(np.unique(mnist.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot individual datapoints as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of datapoint no. 9:\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 189. 190.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 143. 247. 153.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 136. 247. 242.  86.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 192. 252. 187.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  62. 185.\n",
      "  18.   0.   0.   0.   0.  89. 236. 217.  47.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 216. 253.\n",
      "  60.   0.   0.   0.   0. 212. 255.  81.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 206. 252.\n",
      "  68.   0.   0.   0.  48. 242. 253.  89.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 131. 251. 212.\n",
      "  21.   0.   0.  11. 167. 252. 197.   5.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  29. 232. 247.  63.\n",
      "   0.   0.   0. 153. 252. 226.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.  45. 219. 252. 143.   0.\n",
      "   0.   0. 116. 249. 252. 103.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   4.  96. 253. 255. 253. 200. 122.\n",
      "   7.  25. 201. 250. 158.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  92. 252. 252. 253. 217. 252. 252.\n",
      " 200. 227. 252. 231.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  87. 251. 247. 231.  65.  48. 189. 252.\n",
      " 252. 253. 252. 251. 227.  35.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 190. 221.  98.   0.   0.   0.  42. 196.\n",
      " 252. 253. 252. 252. 162.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 111.  29.   0.   0.   0.   0.  62. 239.\n",
      " 252.  86.  42.  42.  14.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  15. 148. 253.\n",
      " 218.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 252. 231.\n",
      "  28.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  31. 221. 251. 129.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 218. 252. 160.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 122. 252.  82.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "As image:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4ffd771a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 9\n",
    "print(\"Value of datapoint no. {}:\\n{}\\n\".format(index,mnist.data[index]))\n",
    "print(\"As image:\\n\")\n",
    "plt.imshow(mnist.data[index].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a little bit simpler (and faster!), we can extract from the data binary subsets, that only contain the data for two selected digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first datapoint now is: \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5' '4' '4' ... '5' '4' '5']\n"
     ]
    }
   ],
   "source": [
    "digit0='4'\n",
    "digit1='5'\n",
    "mnist_bin_data=mnist.data[np.logical_or(mnist.target==digit0,mnist.target==digit1)]\n",
    "mnist_bin_target=mnist.target[np.logical_or(mnist.target==digit0,mnist.target==digit1)]\n",
    "print(\"The first datapoint now is: \\n\")\n",
    "plt.imshow(mnist_bin_data[0].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "plt.show()\n",
    "print(mnist_bin_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 [SVM]:** Split the mnist_bin data into training and test set. Learn different SVM models by varying the kernel functions (SVM). For each configuration, determine the time it takes to learn the model, and the accuracy on the test data. \n",
    "\n",
    "You can get the current time using:\n",
    "\n",
    "`import time` <br>\n",
    "`now = time.time()`\n",
    "\n",
    "*Caution*: for some configurations, learning here can take a little while (several minutes).\n",
    "\n",
    "Using the numpy where() function, one can extract the indices of the test cases that were misclassified: <br>\n",
    "`misclass = np.where(test != predictions)` <br>\n",
    "Inspect some misclassified cases. Do they correspond to hard to recognize digits (also for the human reader)? \n",
    "\n",
    "How do results (time and accuracy) change, depending on whether you consider an 'easy' binary task (e.g., distinguishing '1' and '0'), or a more difficult one (e.g., '4' vs. '5'). \n",
    "\n",
    "Identify one or several good configurations that give a reasonable combination of accuracy and runtime. Use these configurations to perform a full classification of the 10 classes in the original dataset (after split into train/test). Using `sklearn.metrics.confusion_matrix` you can get an overview of all combinations of true and predicted labels. What does this tell you about which digits are easy, and which ones are difficult to recognize, and which ones are most easily confused?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 [SVM]:** Consider how the current data representation \"presents\" the digits to the classifiers, and try to improve this:<br>\n",
    "\n",
    "**a)** Manually design feature functions for which you expect that based on your new features SVM classifiers can achieve a better accuracy than with the original features. Transform the data into your new feature space, and learn new classifiers. What accuracies do you get?\n",
    "\n",
    "\n",
    "**b)** Instead of designing an explicit feature mapping as in **a)**, define a suitable measure of similarity for the digits, and implement that measure as a kernel function. (Optional: verify that the function you have defined actually satisfies the positive-semidefiniteness property.) Use your kernel function as a custom kernel for the SVC classifier.  See http://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html#sphx-glr-auto-examples-svm-plot-custom-kernel-py for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Exercise 1\n",
    "''' Completely dies with 7 and 9, cant make it work :(\n",
    "In the rest of the tasks it performed quite well with really high accuracies, for example 1 and 0 it ran with 99 % accuracy with a test size of 30 %,\n",
    "and 7 misclassifications, ran in 1,72 secs. 6 and 3 it only has 23 misclassification  but runs in 4 times the time( 4 secs), for 4 and 5 it runs in 6 secs but 47 misclassifcations \n",
    "wih sigmoid it took 181secs and had  aclassification of 53% on test data and 51 on training data when comparing 4 and 5\n",
    "rbf was taking so long I had to shut it down. UPDATE** TOOK 180 secs and 53 accuracy...\n",
    "and poly took 12 secs , so basically 4x the time. So Im sticking with linear kernel\n",
    "'''\n",
    "\n",
    "##you have to choose different types of digits in the upward cell\n",
    "import time\n",
    "now = time.time()\n",
    "print(\"alive\")\n",
    "#x: np.ndarray = mnist_bin_data\n",
    "#print(mnist_bin_data.shape)\n",
    "y: np.ndarray = mnist_bin_target\n",
    "\n",
    "trnX, tstX, trnY, tstY = train_test_split(mnist.data, mnist.target, test_size=0.2,random_state=20)\n",
    "\n",
    "\n",
    "print( \"I'm doing stuff no worries\")\n",
    "classifier = SVC(kernel='polynomial')\n",
    "classifier.fit(trnX,trnY)\n",
    "\n",
    "pred_labels_train=classifier.predict(trnX)\n",
    "print(\"Don't worry im training!\")\n",
    "pred_labels=classifier.predict(tstX)\n",
    "\n",
    "misclassified = np.where(tstY != pred_labels)\n",
    "\n",
    "\n",
    "##accuracy\n",
    "print(\"Accuracy test: {}\".format(accuracy_score(tstY,pred_labels)))\n",
    "print(\"Accuracy train: {}\".format(accuracy_score(trnY,pred_labels_train)))\n",
    "print(\"Time required: {}\" .format(time.time()-now))\n",
    "print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "print(confusion_matrix(tstY, pred_labels, labels=np.unique(mnist.target)))\n",
    "\n",
    "print(\"misclassified nr:\" , len(misclassified[0]))\n",
    "#print(\"image misclassified\",plt.imshow(mnist_bin_data[misclassified[0][0]].reshape(28,28),cmap=plt.cm.gray_r))    \n",
    "#### RESULTS  ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---Exercise 2\n",
    "#1st approach try to reshape the data?\n",
    "#normalize the pixels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##exercise 2\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "now = time.time()\n",
    "\n",
    "#x: np.ndarray = mnist_bin_data\n",
    "#y: np.ndarray = mnist_bin_target\n",
    "print(\"don't worry i've just started\")\n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "\n",
    "trnX, tstX, trnY, tstY = train_test_split(mnist.data, mnist.target, test_size=0.3,random_state=20)\n",
    "print(\"don't worry i'm alive\")\n",
    "scaler.fit_transform(trnX)\n",
    "scaler.fit_transform(tstX)\n",
    "\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(trnX, trnY)\n",
    "\n",
    "pred_labels=model.predict(tstX)\n",
    "\n",
    "print(\"Accuracy test: {}\".format(accuracy_score(tstY,pred_labels)))\n",
    "print(\"Accuracy train: {}\".format(accuracy_score(trnY,pred_labels_train)))\n",
    "print(\"Time required: {}\" .format(time.time()-now))\n",
    "\n",
    "#already getting really high accuracy so not really sure  how to increase with the same classifier\n",
    "#couldn't run the entire dataset it gets stuck, waited for a long time...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "now = time.time()\n",
    "\n",
    "def good_kernel(trnX,trnY):\n",
    "    return (np.dot(trnX.T, trnY)/255)\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel=good_kernel)\n",
    "clf.fit(trnX, trnY)\n",
    "\n",
    "pred_labels=model.predict(tstX)\n",
    "\n",
    "print(\"Accuracy test: {}\".format(accuracy_score(tstY,pred_labels)))\n",
    "print(\"Accuracy train: {}\".format(accuracy_score(trnY,pred_labels_train)))\n",
    "print(\"Time required: {}\" .format(time.time()-now))\n",
    "\n",
    "###inear SVM is less prone to overfitting than non-linear.\n",
    "#And you need to decide which kernel to choose based on your situation: if your number of features is really\n",
    "#large compared to the training sample, just use linear kernel; if your number of features\n",
    "#is small, but the training sample is large, you may also need linear kernel but try to add more features;\n",
    "\n",
    "#rbf should work better since it the features are highly non linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
